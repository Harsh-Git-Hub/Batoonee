# -*- coding: utf-8 -*-
"""qa-seq2seq
Author: Harsh Gupta
Created on: 2nd Nov 2020 5:01 PM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_P3NQ5VsbH-X5LXlvcFseeNCJYTRGg-B
"""

import os
import pathlib

import keras
import numpy as np
import tensorflow as tf
from keras import Model
from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, Concatenate
from .attention import Attention


class Seq2Seq:
    def __init__(self, ques, ans, qtk, atk, save_weights=True, save_model=False):
        self.ques = ques
        self.ans = ans
        self.qtk = qtk
        self.atk = atk
        self.ques_enc = None
        self.ans_enc = None
        self.dec = None
        self.encoder_inputs = None
        self.encoder_states = None
        self.decoder_inputs = None
        self.decoder_lstm = None
        self.dec_x = None
        self.decoder_dense = None
        self.model = None
        self.encoder_model = None
        self.decoder_model = None
        self.save_weights = save_weights
        self.save_model = save_model
        self.wfp = "{0}\\saved\\weights\\s2s_weights.h5".format(os.getcwd())
        self.mfp = "{0}\\saved\\models\\s2s.h5".format(os.getcwd())

    def get_encodings(self):
        max_ques_sen_len = 0
        max_ans_sen_len = 0
        num_pairs = len(self.ques)
        for i in range(num_pairs):
            if len(self.ques[i]) > max_ques_sen_len:
                max_ques_sen_len = len(self.ques[i])

            if len(self.ans[i]) > max_ans_sen_len:
                max_ans_sen_len = len(self.ans[i])

        print(max_ques_sen_len, max_ans_sen_len)

        self.ques_enc = np.zeros(shape=(num_pairs, max_ques_sen_len), dtype=np.int16)
        self.ans_enc = np.zeros(shape=(num_pairs, max_ans_sen_len), dtype=np.int16)
        self.dec = np.zeros(shape=(num_pairs, max_ans_sen_len, len(self.atk.word_index) + 1))

        # fill(ques, ans, ques_enc, ans_enc, dec)
        return self.ques_enc, self.ans_enc, self.dec

    def fill_encodings(self):
        for i, (input_text, target_text) in enumerate(zip(self.ques, self.ans)):
            for t, word_index in enumerate(input_text):
                self.ques_enc[i, t] = word_index

            for t, word_index in enumerate(target_text):
                self.ans_enc[i, t] = word_index
                if t > 0:
                    self.dec[i, t - 1, word_index] = 1

    def __check_existing_models(self):
        if pathlib.Path(self.mfp).is_file():
            self.model = keras.models.load_model(self.mfp)
            return True
        return False

    def __check_existing_weights(self):
        if pathlib.Path(self.wfp).is_file():
            if self.model is not None:
                self.model.load_weights(self.wfp)

    def build_model(self, num_encoder_tokens, num_decoder_tokens, latent_dim=50):
        # if self.__check_existing_models():
        #     return self.model, True

        self.__check_existing_weights()

        # Define an input sequence and process it setting up an encoder
        self.encoder_inputs = Input(shape=(None,))
        x = Embedding(num_encoder_tokens, latent_dim)(self.encoder_inputs)
        encoder_forward = LSTM(latent_dim, return_state=True, dropout=0.1,
                               kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.0001),
                               bias_regularizer=keras.regularizers.l2(l2=0.001))
        encoder_backward = LSTM(latent_dim, return_state=True, go_backwards=True, dropout=0.1,
                                kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001),
                                bias_regularizer=keras.regularizers.l2(l2=0.01))
        encoder = Bidirectional(encoder_forward, backward_layer=encoder_backward)
        encoder_outputs, encoder_forward_h, encoder_forward_c, encoder_backward_h, encoder_backward_c = encoder(x)
        state_h = Concatenate()([encoder_forward_h, encoder_backward_h])
        state_c = Concatenate()([encoder_forward_c, encoder_backward_c])
        self.encoder_states = [state_h, state_c]

        # attn = Attention()
        # attn, attn_states = attn(encoder_outputs)

        # Set up the decoder, using `encoder_states` as initial state.
        self.decoder_inputs = Input(shape=(None,))
        self.dec_x = Embedding(num_decoder_tokens, latent_dim)
        x = self.dec_x(self.decoder_inputs)
        self.decoder_lstm = LSTM(latent_dim * 2, return_state=True, return_sequences=True)
        x, _, _ = self.decoder_lstm(x, initial_state=self.encoder_states)
        self.decoder_dense = Dense(num_decoder_tokens, activation='softmax')
        decoder_outputs = self.decoder_dense(x)

        # Define the model that will turn `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
        self.model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)

        if self.save_model:
            self.model.save(self.mfp)
        if self.save_weights:
            self.model.save_weights(self.wfp)

        return self.model, False

    def train(self, inputs, output, optimizer='adam', metrics=None, callbacks=None, batch_size=50, epochs=100,
              val_split=0.15, verbose=1, shuffle=True):
        if metrics is None:
            metrics = ["categorical_accuracy", tf.keras.metrics.CategoricalCrossentropy()]
        if callbacks is None:
            callbacks = []

        self.model.compile(optimizer=optimizer,
                           loss='categorical_crossentropy',
                           metrics=metrics)

        self.model.fit(inputs, output,
                       batch_size=batch_size,
                       epochs=epochs,
                       validation_split=val_split,
                       callbacks=callbacks,
                       verbose=verbose,
                       shuffle=shuffle)

    def infer(self, latent_dim=50):
        self.encoder_model = Model(self.encoder_inputs, self.encoder_states)

        decoder_state_input_h = Input(shape=(latent_dim * 2,))
        decoder_state_input_c = Input(shape=(latent_dim * 2,))

        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
        fdec_x = self.dec_x(self.decoder_inputs)
        decoder_outputs, state_h, state_c = self.decoder_lstm(fdec_x, initial_state=decoder_states_inputs)
        decoder_states = [state_h, state_c]
        decoder_outputs = self.decoder_dense(decoder_outputs)
        self.decoder_model = Model(
            [self.decoder_inputs] + decoder_states_inputs,
            [decoder_outputs] + decoder_states
        )

        return self.encoder_model, self.decoder_model

    def decode_sequence(self, input_seq, decoder_input_data):
        states_value = self.encoder_model.predict(input_seq)
        target_seq = np.zeros(shape=(1, 1))
        target_seq[0, 0] = self.qtk.word_index['sos']
        stop_condition = False
        decoded_sentence = ''
        while not stop_condition:
            output_tokens, state_h, state_c = self.decoder_model.predict([target_seq] + states_value)

            sampled_token_index = np.argmax(output_tokens[0, -1, :])
            sampled_word = self.atk.index_word[sampled_token_index]

            decoded_sentence += ' ' + sampled_word
            if sampled_word == 'eos' or len(decoded_sentence) > decoder_input_data.shape[1]:
                stop_condition = True

            target_seq = np.zeros((1, 1))
            target_seq[0, 0] = sampled_token_index
            states_value = [state_h, state_c]

        # return decoded_sentence.rsplit(' ', 1)[0]
        return decoded_sentence
